{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1388df5d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seed random number generator\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Splitting data into features and labels\n",
    "X = data['tweet'].values\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating DataFrame for training and validation sets\n",
    "train_data = pd.DataFrame({'tweet': X_train})\n",
    "test_data = pd.DataFrame({'tweet': X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @FunSizedYogi: @TheBlackVoice well how else...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funny thing is....it's not just the people doi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S/o that real bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>The last at-bat at Yankee Stadium. Thanks for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>@_bradleey LMFAOOOO yooo I lost my elevator pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>#porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>RT @JennyJohnsonHi5: Just when I thought Justi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>bitches ain&amp;#8217;t shit, and they ain&amp;#8217;t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet\n",
       "0      RT @FunSizedYogi: @TheBlackVoice well how else...\n",
       "1      Funny thing is....it's not just the people doi...\n",
       "2      RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...\n",
       "3      @Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...\n",
       "4                                    S/o that real bitch\n",
       "...                                                  ...\n",
       "19821  The last at-bat at Yankee Stadium. Thanks for ...\n",
       "19822  @_bradleey LMFAOOOO yooo I lost my elevator pa...\n",
       "19823  #porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...\n",
       "19824  RT @JennyJohnsonHi5: Just when I thought Justi...\n",
       "19825  bitches ain&#8217;t shit, and they ain&#8217;t...\n",
       "\n",
       "[19826 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>934 8616\\ni got a missed call from yo bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KINGTUNCHI_: Fucking with a bad bitch you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @eanahS__: @1inkkofrosess lol my credit ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Maxin_Betha Wipe the cum out of them faggo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niggas cheat on they bitch and don't expect no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>@GrizzboAdams @wyattnuckels haha ight nig calm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>When you see kids being bad &amp;amp; their parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>This bitch done blew my high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>Fat Trel that niggah &amp;#128076;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>Neverrr, play me for stupid because I'm far fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4957 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet\n",
       "0           934 8616\\ni got a missed call from yo bitch\n",
       "1     RT @KINGTUNCHI_: Fucking with a bad bitch you ...\n",
       "2     RT @eanahS__: @1inkkofrosess lol my credit ain...\n",
       "3     RT @Maxin_Betha Wipe the cum out of them faggo...\n",
       "4     Niggas cheat on they bitch and don't expect no...\n",
       "...                                                 ...\n",
       "4952  @GrizzboAdams @wyattnuckels haha ight nig calm...\n",
       "4953  When you see kids being bad &amp; their parent...\n",
       "4954                       This bitch done blew my high\n",
       "4955                     Fat Trel that niggah &#128076;\n",
       "4956  Neverrr, play me for stupid because I'm far fr...\n",
       "\n",
       "[4957 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Model\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r4-target\")\n",
    "# # This one said non-hate to a lot of hate speech as far as I have tried.\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "target_model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>934 8616\\ni got a missed call from yo bitch</td>\n",
       "      <td>[0.5540308952331543, 0.4459691643714905]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KINGTUNCHI_: Fucking with a bad bitch you ...</td>\n",
       "      <td>[0.3774803876876831, 0.6225196719169617]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @eanahS__: @1inkkofrosess lol my credit ain...</td>\n",
       "      <td>[0.9582732319831848, 0.04172678291797638]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Maxin_Betha Wipe the cum out of them faggo...</td>\n",
       "      <td>[0.09527343511581421, 0.904726505279541]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Niggas cheat on they bitch and don't expect no...</td>\n",
       "      <td>[0.11824338138103485, 0.881756603717804]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4952</th>\n",
       "      <td>@GrizzboAdams @wyattnuckels haha ight nig calm...</td>\n",
       "      <td>[0.09661741554737091, 0.9033825397491455]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>When you see kids being bad &amp;amp; their parent...</td>\n",
       "      <td>[0.35169070959091187, 0.6483092904090881]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>This bitch done blew my high</td>\n",
       "      <td>[0.21178992092609406, 0.7882100939750671]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4955</th>\n",
       "      <td>Fat Trel that niggah &amp;#128076;</td>\n",
       "      <td>[0.08707571029663086, 0.9129242300987244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4956</th>\n",
       "      <td>Neverrr, play me for stupid because I'm far fr...</td>\n",
       "      <td>[0.2171916663646698, 0.7828083634376526]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4957 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  \\\n",
       "0           934 8616\\ni got a missed call from yo bitch   \n",
       "1     RT @KINGTUNCHI_: Fucking with a bad bitch you ...   \n",
       "2     RT @eanahS__: @1inkkofrosess lol my credit ain...   \n",
       "3     RT @Maxin_Betha Wipe the cum out of them faggo...   \n",
       "4     Niggas cheat on they bitch and don't expect no...   \n",
       "...                                                 ...   \n",
       "4952  @GrizzboAdams @wyattnuckels haha ight nig calm...   \n",
       "4953  When you see kids being bad &amp; their parent...   \n",
       "4954                       This bitch done blew my high   \n",
       "4955                     Fat Trel that niggah &#128076;   \n",
       "4956  Neverrr, play me for stupid because I'm far fr...   \n",
       "\n",
       "                                          label  \n",
       "0      [0.5540308952331543, 0.4459691643714905]  \n",
       "1      [0.3774803876876831, 0.6225196719169617]  \n",
       "2     [0.9582732319831848, 0.04172678291797638]  \n",
       "3      [0.09527343511581421, 0.904726505279541]  \n",
       "4      [0.11824338138103485, 0.881756603717804]  \n",
       "...                                         ...  \n",
       "4952  [0.09661741554737091, 0.9033825397491455]  \n",
       "4953  [0.35169070959091187, 0.6483092904090881]  \n",
       "4954  [0.21178992092609406, 0.7882100939750671]  \n",
       "4955  [0.08707571029663086, 0.9129242300987244]  \n",
       "4956   [0.2171916663646698, 0.7828083634376526]  \n",
       "\n",
       "[4957 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For validation w.r.t. the target model (but does it technically increases the number of queries?)\n",
    "def get_label(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Query the target model\n",
    "    with torch.no_grad():\n",
    "        target_outputs = target_model(**inputs)\n",
    "\n",
    "    target_labels = target_outputs.logits.softmax(dim=1).tolist()[0]\n",
    "    return target_labels\n",
    "\n",
    "test_data['label'] = test_data['tweet'].apply(get_label) # ~ 2/3 mins\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained embeddings\n",
    "# Load pre-trained GloVe embeddings\n",
    "embed_dim = 100\n",
    "glove = vocab.GloVe(name='6B', dim=embed_dim)\n",
    "\n",
    "# Get the vocabulary from the pre-trained embeddings\n",
    "glove_vocab = glove.stoi  # Dictionary mapping words to their indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Model class\n",
    "class HateSpeechGRU(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, hidden_size, output_dim, dropout):\n",
    "        super(HateSpeechGRU, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_dim)  # * 2 for bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  # text: [batch size, sent len]\n",
    "        output, hidden = self.gru(embedded)  # output: [batch size, sent len, hidden_size * num_directions]\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # concatenate the final forward and backward hidden states\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)  # output: [batch size, output dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Model\n",
    "# Define hyperparameters\n",
    "pretrained_embeddings = glove.vectors # Create a matrix of pre-trained embeddings\n",
    "hidden_size = 128  # Size of hidden states in the GRU\n",
    "output_dim = 2  # Number of output classes (binary classification)\n",
    "dropout = 0.5  # Dropout probability\n",
    "\n",
    "clone_model = HateSpeechGRU(pretrained_embeddings, hidden_size, output_dim, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19826, 384])\n",
      "torch.Size([19826, 384])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @FunSizedYogi: @TheBlackVoice well how else...</td>\n",
       "      <td>[-0.04517171159386635, 0.13797102868556976, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funny thing is....it's not just the people doi...</td>\n",
       "      <td>[0.050247207283973694, 0.0015786909498274326, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...</td>\n",
       "      <td>[-0.11634157598018646, 0.04812648147344589, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...</td>\n",
       "      <td>[-0.15736792981624603, -0.020282931625843048, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S/o that real bitch</td>\n",
       "      <td>[-0.11650454998016357, -0.026171348989009857, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19821</th>\n",
       "      <td>The last at-bat at Yankee Stadium. Thanks for ...</td>\n",
       "      <td>[-0.027169303968548775, 0.11435810476541519, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19822</th>\n",
       "      <td>@_bradleey LMFAOOOO yooo I lost my elevator pa...</td>\n",
       "      <td>[-0.0282177422195673, -0.054308995604515076, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19823</th>\n",
       "      <td>#porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...</td>\n",
       "      <td>[0.0208604633808136, -0.0533132366836071, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19824</th>\n",
       "      <td>RT @JennyJohnsonHi5: Just when I thought Justi...</td>\n",
       "      <td>[0.009069043211638927, -0.011038385331630707, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19825</th>\n",
       "      <td>bitches ain&amp;#8217;t shit, and they ain&amp;#8217;t...</td>\n",
       "      <td>[-0.0677453801035881, -0.016362426802515984, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  \\\n",
       "0      RT @FunSizedYogi: @TheBlackVoice well how else...   \n",
       "1      Funny thing is....it's not just the people doi...   \n",
       "2      RT @winkSOSA: \"@AintShitSweet__: \"@Rakwon_OGOD...   \n",
       "3      @Jbrendaro30 @ZGabrail @ramsin1995 @GabeEli8 @...   \n",
       "4                                    S/o that real bitch   \n",
       "...                                                  ...   \n",
       "19821  The last at-bat at Yankee Stadium. Thanks for ...   \n",
       "19822  @_bradleey LMFAOOOO yooo I lost my elevator pa...   \n",
       "19823  #porn,#android,#iphone,#ipad,#sex,#xxx, | #Ana...   \n",
       "19824  RT @JennyJohnsonHi5: Just when I thought Justi...   \n",
       "19825  bitches ain&#8217;t shit, and they ain&#8217;t...   \n",
       "\n",
       "                                               embedding  \n",
       "0      [-0.04517171159386635, 0.13797102868556976, 0....  \n",
       "1      [0.050247207283973694, 0.0015786909498274326, ...  \n",
       "2      [-0.11634157598018646, 0.04812648147344589, 0....  \n",
       "3      [-0.15736792981624603, -0.020282931625843048, ...  \n",
       "4      [-0.11650454998016357, -0.026171348989009857, ...  \n",
       "...                                                  ...  \n",
       "19821  [-0.027169303968548775, 0.11435810476541519, 0...  \n",
       "19822  [-0.0282177422195673, -0.054308995604515076, 0...  \n",
       "19823  [0.0208604633808136, -0.0533132366836071, 0.01...  \n",
       "19824  [0.009069043211638927, -0.011038385331630707, ...  \n",
       "19825  [-0.0677453801035881, -0.016362426802515984, 0...  \n",
       "\n",
       "[19826 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained Sentence Transformer model\n",
    "sent_transformer = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "# Create the embeddings for the training data\n",
    "# ~ 30 secs\n",
    "train_embeddings = sent_transformer.encode(train_data['tweet'].tolist(), convert_to_tensor=True).tolist()\n",
    "\n",
    "#normalize the embeddings\n",
    "train_embeddings = torch.tensor(train_embeddings).to(device)\n",
    "print(train_embeddings.shape)\n",
    "train_embeddings = nn.functional.normalize(train_embeddings, p=2, dim=1)\n",
    "print(train_embeddings.shape)\n",
    "\n",
    "train_data['embedding'] = train_embeddings.cpu().tolist()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table to store the previous queries (for wise query selection later)\n",
    "table = pd.DataFrame({\n",
    "    'tweet': [],\n",
    "    'embedding': [],\n",
    "    't_out': [],\n",
    "    'c_out': []\n",
    "})\n",
    "\n",
    "alpha = 0.5 # Weight for 'dissimilariy with the previous queries' term from the formula\n",
    "beta = 1-alpha # Weight for 'similarity with the previous queries that had hish disagreement' term from the formula\n",
    "\n",
    "\n",
    "# Train the clone model\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(clone_model.parameters(), lr=0.001)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Define number of epochs\n",
    "epoch = 0\n",
    "\n",
    "loss_vals = []\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "    \n",
    "while True:\n",
    "    clone_model.train()\n",
    "    \n",
    "    if table.shape[0] == 0: # Cold start for the first query (can be random)\n",
    "        idxs = torch.randperm(len(train_data))[:batch_size].tolist()\n",
    "    else:\n",
    "        # Wise query selection\n",
    "        # Calculate the cosine similarity between the embeddings of the training data and the table\n",
    "        similarities = util.cos_sim(torch.tensor(train_data['embedding'].tolist()), torch.tensor(table['embedding'].tolist()))\n",
    "        \n",
    "        # Calculate the average cosine similarity for each training data\n",
    "        # 'dissimilariy with the previous queries'\n",
    "        avg_similarities = similarities.max(dim=1).values\n",
    "\n",
    "        # 'similarity with the previous queries that had hish disagreement'\n",
    "        disagreement = torch.tensor((abs(table['c_out'] - table['t_out'])).tolist())\n",
    "        \n",
    "        similarities = similarities * disagreement\n",
    "        avg_wrt_disagreement = similarities.mean(dim=1)\n",
    "\n",
    "        # Calculating Z-scores for avg_similarities and avg_wrt_disagreement\n",
    "        avg_similarities_z = (avg_similarities - avg_similarities.mean()) / avg_similarities.std()\n",
    "        avg_wrt_disagreement_z = (avg_wrt_disagreement - avg_wrt_disagreement.mean()) / avg_wrt_disagreement.std()\n",
    "\n",
    "        # Calculate the normalized formula for each training data\n",
    "        formula = alpha * (-avg_similarities_z) + beta * avg_wrt_disagreement_z\n",
    "        \n",
    "        # Get the index of the training data with the lowest average cosine similarity\n",
    "        idxs = formula.argsort(descending=True)[:batch_size].tolist()\n",
    "\n",
    "        print('argsorted formula', formula.sort(descending=True)[:batch_size])\n",
    "\n",
    "    input_texts = train_data.iloc[idxs]['tweet']\n",
    "\n",
    "    # Check if the input_texts are already queried\n",
    "    will_added = [True] * len(input_texts)\n",
    "\n",
    "    count = 0\n",
    "    for t in range(len(input_texts)):\n",
    "        if input_texts.iloc[t] in table['tweet'].tolist():\n",
    "            will_added[t] = False\n",
    "            count += 1\n",
    "    print('girdigi sayi', count)\n",
    "            \n",
    "\n",
    "    input_embeds = train_data.iloc[idxs]['embedding']\n",
    "\n",
    "    # inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = tokenizer(input_texts.tolist(), return_tensors=\"pt\", padding=True, truncation=False).to(device)\n",
    "\n",
    "    # Query the target model\n",
    "    with torch.no_grad():\n",
    "        target_outputs = target_model(**input_tokens)\n",
    "\n",
    "    target_labels = target_outputs.logits.softmax(dim=1).tolist()\n",
    "\n",
    "    # Conver input_text to tensor using glove_vocab\n",
    "    input_glove = [[glove_vocab[word] if word in glove_vocab else glove_vocab['unk'] for word in input_text.split()] for input_text in input_texts]\n",
    "    max_len = max([len(input_text) for input_text in input_glove])\n",
    "    input_glove = [input_text + [glove_vocab['unk']] * (max_len - len(input_text)) for input_text in input_glove]\n",
    "    input_glove = torch.tensor(input_glove, dtype=torch.long).to(device)\n",
    "\n",
    "    target_outputs = torch.tensor(target_labels).to(device)\n",
    "\n",
    "    # Forward pass through the clone model\n",
    "    with autocast():\n",
    "        clone_logits = clone_model(input_glove)\n",
    "        clone_outputs = torch.softmax(clone_logits, dim=-1)\n",
    "        loss = criterion(clone_outputs, target_outputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "\n",
    "    # Update model weights considering the scaled gradients\n",
    "    scaler.step(optimizer)\n",
    "\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "\n",
    "    input_texts = input_texts[will_added]\n",
    "    input_embeds = input_embeds[will_added]\n",
    "    target_outputs = target_outputs[will_added]\n",
    "    clone_outputs = clone_outputs[will_added]\n",
    "\n",
    "    # Add row to table\n",
    "    row = pd.DataFrame({'tweet': input_texts, 'embedding': input_embeds, 't_out': target_outputs[:,0].tolist(), 'c_out': clone_outputs[:,0].tolist()})\n",
    "    table = pd.concat([table, row], ignore_index=True)\n",
    "\n",
    "    # Evaluation\n",
    "    clone_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # Updating the table\n",
    "        input_glove_2 = [[glove_vocab[word] if word in glove_vocab else glove_vocab['unk'] for word in input_text.split()] for input_text in table['tweet'].tolist()]\n",
    "        max_len = max([len(input_text) for input_text in input_glove_2])\n",
    "        input_glove_2 = [input_text + [glove_vocab['unk']] * (max_len - len(input_text)) for input_text in input_glove_2]\n",
    "\n",
    "        input_glove_2 = torch.tensor(input_glove_2, dtype=torch.long).to(device)\n",
    "\n",
    "        with autocast():\n",
    "            clone_logits_2 = clone_model(input_glove_2)\n",
    "            clone_outputs_2 = torch.softmax(clone_logits_2, dim=-1)\n",
    "\n",
    "        table['c_out'] = clone_outputs_2[:,0].tolist()\n",
    "        \n",
    "        # Validation set\n",
    "        inputs = [[glove_vocab[word] if word in glove_vocab else glove_vocab['unk'] for word in txt.split()] for txt in test_data['tweet'].tolist()] # TODO: Do once, don't do it in every epoch\n",
    "        max_len = max([len(txt) for txt in inputs])\n",
    "        inputs = [txt + [glove_vocab['unk']] * (max_len - len(txt)) for txt in inputs]\n",
    "\n",
    "\n",
    "        test_pred = clone_model(torch.tensor(inputs, device=device))\n",
    "        test_loss = criterion(test_pred, torch.tensor(test_data['label'].tolist(), device=device))\n",
    "\n",
    "    loss_vals.append(test_loss.item())\n",
    "    print(f'Epoch [{epoch + 1}], Loss: {test_loss.item()}')\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "    print('table size ' ,table.shape)\n",
    "    print('train_data size ' ,input_texts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
